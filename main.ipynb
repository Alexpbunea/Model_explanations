{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#First imports\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Reading the dataset\n",
    "\n",
    "train_file_path = 'titanic.csv' #adjust it for drive\n",
    "original_df = pd.read_csv(train_file_path)\n",
    "original_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Doing some changes\n",
    "\n",
    "df = original_df.copy()\n",
    "df.rename(columns={\"PassengerId\": \"ID\"}, inplace=True)\n",
    "df = df.drop(columns=[\"Name\", \"Ticket\", \"Cabin\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Embarked\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df[\"MissAge\"] = df['Age'].isna().astype(int)\n",
    "df.fillna({'Age':0}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sex_trans = LabelEncoder()\n",
    "df[\"Sex\"] = sex_trans.fit_transform(df[\"Sex\"])\n",
    "\n",
    "Emb_trans = LabelEncoder()\n",
    "df[\"Embarked\"] = sex_trans.fit_transform(df[\"Embarked\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Nomalize = StandardScaler()\n",
    "Nomalize_cols = [\"Age\", \"Fare\"]\n",
    "df[Nomalize_cols] = Nomalize.fit_transform(df[Nomalize_cols])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Survived])\n",
    "y = df[\"Survived]\n",
    "\n",
    "IDs = X[\"ID\"].values.reshape(-1,1).astype(np.float32)\n",
    "IDs = IDs / 1e7\n",
    "\n",
    "X = X.drop(columns=[\"ID\"]).values.astype(np.float32)\n",
    "X = np.hstack((X, IDs))\n",
    "y = to_categorical(y.values, num_classes=2)\n",
    "\n",
    "X.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    layers.Dense(32, activation=\"relu\", input_shape=(9,)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(2, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(train_ds.batch(32), epochs=10, validation_data=test_ds.batch(32), verbose=2)\n",
    "model.evaluate(test_ds.batch(32), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from deel.influenciae.common import InfluenceModel, ExactIHVP\n",
    "from deel.influenciae.influence import FirstOrderInfluenceCalculator\n",
    "from deel.influenciae.utils import ORDER\n",
    "from deel.influenciae.trac_in import TracIn\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "unreduced_loss = BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "influence_model = InfluenceModel(model, start_layer=-1, loss_function=unreduced_loss)\n",
    "\n",
    "ihvp_calculator = ExactIHVP(influence_model, train_dataset=train_ds.shuffle(100).batch(4))\n",
    "influence_calculator = FirstOrderInfluenceCalculator(influence_model, train_ds.batch(8), ihvp_calculator)\n",
    "\n",
    "samples_to_explain = test_ds.take(5).batch(1)\n",
    "explanation_ds = influence_calculator.top_k(samples_to_explain, train_ds.batch(8), k=3, order=ORDER.DESCENDING)\n",
    "\n",
    "for (sample, label), top_k_values, top_k_samples in explanation_ds.as_numpy_iterator():\n",
    "    sample_id = round(sample[0][-1] * 1e7)\n",
    "    sample_original = original_df[original_df[\"PassengerId\"] == sample_id]\n",
    "\n",
    "    print(f\"\\nTest Sample ID: {sample_id}\")\n",
    "    print(\"Original Sample from DataFrame: \")\n",
    "    print(sample_original[[\"Survived]])\n",
    "\n",
    "    influential_ids = [round(s[-1] * 1e7) for s in top_k_samples[0]]\n",
    "    for i, (inf_id, score) in enumerate(zip(influential_ids, top_k_values[0])):\n",
    "        inf_sample_original = original_df[original_df[\"PassengerId\"] == inf_id]\n",
    "        \n",
    "        print(f\"Influential Sample {i+1} -> ID: {inf_id}, Influence Score: {score})\n",
    "        print(inf_sample_original[[\"Survived]])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
